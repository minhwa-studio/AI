import os
import torch
from diffusers import StableDiffusionImg2ImgPipeline
from PIL import Image
import cv2
import numpy as np
from diffusers.utils import load_image
from torchvision.utils import make_grid
from datetime import datetime
from torchvision.transforms import ToTensor, ToPILImage


# ------------------------------------------------
# 1. GPU ë° ëª¨ë¸ ë¡œë“œ
# ------------------------------------------------
print("GPU ë° ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤... â³")

device = "cuda" if torch.cuda.is_available() else "cpu"

pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True
).to(device)

lora_path = "sd15_lora_minhwa"
pipeline.load_lora_weights(lora_path, weight_name="pytorch_lora_weights.safetensors")

# ------------------------------------------------
# 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ìŠ¤íƒ€ì¼ ë³€í™˜
# ------------------------------------------------
print("\ní…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ìŠ¤íƒ€ì¼ì„ ì ìš©í•©ë‹ˆë‹¤. ğŸ–¼ï¸")

test_image_path = "./image/test3.jpg"
if not os.path.exists(test_image_path):
    print(f"âŒ ì˜¤ë¥˜: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ '{test_image_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

init_image = load_image(test_image_path).convert("RGB")
init_image = init_image.resize((512, 512))

prompt = "a painting of minhwa, a traditional Korean painting, ink wash painting, soft watercolor style, subtle colors, delicate brushstrokes, traditional folk art, monochromatic"
negative_prompt = "japanese style, chinese style, ukiyo-e, sumi-e, western art, European style, masterpiece, oil painting, portrait, lowres, bad anatomy, bad hands, cropped, worst quality, deformed"

# ê·¸ë¦¬ë“œ ìƒì„± ë° ì£¼ìš” íŒŒë¼ë¯¸í„° ì¡°ì •
# [ strength ê°’ ì„¤ëª… ]
# - 0.5~0.7: ì›ë³¸ êµ¬ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìƒë‹¹ ë¶€ë¶„ ë³€ê²½ (ì¶”ì²œ ì‹œì‘ ê°’)
# - 0.8~0.9: ì›ë³¸ êµ¬ë„ë¥¼ ë§ì´ ë³€ê²½í•˜ë©´ì„œ ìŠ¤íƒ€ì¼ ê°•í•˜ê²Œ ì ìš©
strengths_to_test = [0.4,0.6,0.8]

# [ num_inference_steps ê°’ ì„¤ëª… ]
# - 10~20: ë¹ ë¥¸ ìƒì„±. ì´ë¯¸ì§€ê°€ ê±°ì¹ ê±°ë‚˜ ëœ ì„ ëª…
# - 25~30: ì¼ë°˜ì ì´ê³  ê· í˜• ì¡íŒ í’ˆì§ˆ (ê¸°ë³¸ê°’)
# - 40~50: ê³ í’ˆì§ˆ. ìƒì„± ì‹œê°„ ì¦ê°€
steps_to_test = [20,30,40]

# [ guidance_scale ê°’ ì„¤ëª… ]
# - 5~8: í”„ë¡¬í”„íŠ¸ ì˜í–¥ë ¥ì´ ì ê³ , ì›ë³¸ ëª¨ë¸ì˜ ìŠ¤íƒ€ì¼ì„ ë”°ë¦„
# - 9~15: í”„ë¡¬í”„íŠ¸ì˜ ì˜í–¥ì„ ê°•í•˜ê²Œ ë°›ì•„ ì›í•˜ëŠ” ìŠ¤íƒ€ì¼ì„ êµ¬í˜„ (ì¶”ì²œ ì‹œì‘ ê°’)
scales_to_test = [9,11,13]

all_generated_images = []
generator = torch.Generator(device).manual_seed(100)

# ì´ë¯¸ì§€ ë³€í™˜ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
transform_to_tensor = ToTensor()

print("ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
for s in strengths_to_test:
    for num_steps in steps_to_test:
        for g_scale in scales_to_test:
            print(f"-> strength: {s}, steps: {num_steps}, guidance_scale: {g_scale}")
            output = pipeline(
                prompt=prompt,
                negative_prompt=negative_prompt,
                image=init_image,
                strength=s,
                num_inference_steps=num_steps,
                guidance_scale=g_scale,
                generator=generator
            )
            # PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜ í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            tensor_image = transform_to_tensor(output.images[0])
            all_generated_images.append(tensor_image)

# ê·¸ë¦¬ë“œ ì´ë¯¸ì§€ ìƒì„± ë° ì €ì¥
grid_image = make_grid(all_generated_images, nrow=len(scales_to_test) * len(steps_to_test), padding=5)

# íŒŒì¼ëª… ìë™ ìƒì„±
base_name = os.path.splitext(os.path.basename(test_image_path))[0]
output_dir = "./image/result"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

output_image_path = os.path.join(output_dir, f"{base_name}_params_grid.png")

# í…ì„œì¸ ê·¸ë¦¬ë“œ ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
to_pil_image = ToPILImage()
pil_grid_image = to_pil_image(grid_image)
pil_grid_image.save(output_image_path)

print(f"\në‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ìƒì„±ëœ ê·¸ë¦¬ë“œ ì´ë¯¸ì§€ê°€ '{output_image_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. âœ…")